{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenStreetMap (San Francisco, CA, USA) Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: \n",
    "*OpenStreetMap is a map of the world created by people like you and free to use under an open license*\n",
    "\n",
    "OpenStreetMap is a community based curated map containing data about roads, trails, cafes, train stations, etc. around the world.  Objective of this project is to showcase data wrangling steps when working with large datasets.  My son recently mentioned he wanted to visit San Francisco and I've only been there once myself a long time ago.  Thus, the specific area of focus will be one of America's largest cities, San Francisco, California.\n",
    "\n",
    "[OpenStreepMap Relation:  San Francisco (111968)](http://www.openstreetmap.org/relation/111968)\n",
    "\n",
    "[Original San Francisco, California Raw OpenStreetMap dataset from Mapzen](https://mapzen.com/data/metro-extracts/metro/san-francisco_california/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Data Wrangling Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import cerberus\n",
    "import schema\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how big this dataset is of San Francisco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 6C32-8596\n",
      "\n",
      " Directory of C:\\Users\\ctngu\\Google Drive\\Udacity\\Data Wrangling\n",
      "\n",
      "\n",
      " Directory of C:\\Users\\ctngu\\Google Drive\\Udacity\\Data Wrangling\n",
      "\n",
      "09/10/2017  02:20 AM     1,410,642,126 san-francisco_california.osm\n",
      "               1 File(s)  1,410,642,126 bytes\n",
      "               0 Dir(s)  29,087,682,560 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls -l san-francisco_california.osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's a whopping 1.4 GB file.  That's almost 20% the size of my iTunes library.  How about we just take a sample of this dataset, that way doing an overview of the data will take a lot less time trying to go through programmatically.  The following code will go through the original dataset and take every 25th data point and write it into a new file we'll call sample_sf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import xml.etree.cElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"san-francisco_california.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"udacity_sample_sf.osm\"\n",
    "\n",
    "k = 125 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampled dataset we've created is a lot smaller, from **~1.4 GB** to **~0.057 GB**.  Much easier to work with, especially for my modest laptop.  Would be nice to have a supercomputer though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 6C32-8596\n",
      "\n",
      " Directory of C:\\Users\\ctngu\\Google Drive\\Udacity\\Data Wrangling\n",
      "\n",
      "\n",
      " Directory of C:\\Users\\ctngu\\Google Drive\\Udacity\\Data Wrangling\n",
      "\n",
      "09/10/2017  11:36 PM        57,009,542 sample_sf.osm\n",
      "               1 File(s)     57,009,542 bytes\n",
      "               0 Dir(s)  29,089,316,864 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls -l sample_sf.osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to audit like you bought it.  \n",
    "*I know, its a pretty lame rhyme...*\n",
    "\n",
    "Some background info, this datafile is in XML, which is a format that is designed to mark, store, and transport data using \"tags\" to denote specific information within it.  One thing a person can do to get an idea of the type of information he/she could be working with is by counting the tags, thereby previewing it so to speak.  For the OpenStreetMap, key tags are nodes, ways, and relations. \n",
    " \n",
    "* **nodes**: consists of a single point in space defined by its latitude, longitude and node id.  In other words, you know your favorite fast-food joint?  Yea, it describes the location of that.  \n",
    "* **way**: an ordered list of nodes which conceptually serves to describe a road, perimeter or area.  \n",
    "* **relation**: an ordered list of nodes, ways and/or other relations.  This serves to group these pieces of information by relationships.  For example, think of your home as the starting node and all the places you commonly visit; mall, work, Starbucks, Hot Yoga!, strip ba....yea, all that.  A relation is conceptually grouping related points in space.  \n",
    "___\n",
    "### Number of Tags and Tag Types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_tags(filename):\n",
    "    \"\"\"\n",
    "    counts the number of tags within the XML filename and \n",
    "    returns a dictionary of tags and the total count of each tag type found.\n",
    "    \"\"\"\n",
    "    tags = {}\n",
    "    for ev, element in ET.iterparse(filename):\n",
    "        tag = element.tag\n",
    "        if tag not in tags.keys():\n",
    "            tags[tag] = 1\n",
    "        else:\n",
    "            tags[tag] = tags[tag]+1\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'member': 2696,\n",
       " 'nd': 311979,\n",
       " 'node': 265094,\n",
       " 'osm': 1,\n",
       " 'relation': 306,\n",
       " 'tag': 82096,\n",
       " 'way': 33061}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tags('sample_sf.osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly quite a lot of places to go to in one of America's largest cities.\n",
    "\n",
    "* 265,094 nodes\n",
    "* 33,061 ways\n",
    "* 206 relations\n",
    "___\n",
    "### Tag Checks and Problematic Characters:\n",
    "Now, going into this dataset one thing we know for sure is that this the OSM data is a community-based project with contributed data provided manually by others like ourselves...Homo Sapiens.  In my life thus far, I've learned, besides blue cheese being the best cheese out there, that we humans are prone to making errors.  So, it wouldn't hurt to check for unexpected characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')  # tags that contain ONLY lowercase letters and are valid.\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$') # tags that are valid but with a colon.\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]') # tags with problematic characters.  \n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        # YOUR CODE HERE\n",
    "        if lower.match(element.attrib['k']):\n",
    "            keys['lower'] += 1\n",
    "        elif lower_colon.match(element.attrib['k']):\n",
    "            keys['lower_colon'] += 1\n",
    "        elif problemchars.match(element.attrib['k']):\n",
    "            keys['problemchars'] += 1\n",
    "        else:\n",
    "            keys['other'] += 1  # other for tags that do not fall into the other three categories.\n",
    "            \n",
    "    return keys\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lower': 57858, 'lower_colon': 23243, 'other': 995, 'problemchars': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_map('sample_sf.osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us an idea of what kind of characters we can expect in our data values.  Fortunately, our dataset here has **0** tags with problem characters.  Still that doesn't mean we should assume there aren't any unwanted errors in the data.  It's always good to be through.\n",
    "___\n",
    "### Unique Users:\n",
    "Reversing a bit, knowing that OpenStreetMap is a community project I'm actually a bit curious as to exactly who and how many users contributed to this San Francisco dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_user(element):\n",
    "    return\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    \"\"\"\n",
    "    Iterate through the OSM file in search for 'user' keys \n",
    "    and return a set containing a list of each unique user.\n",
    "    Return the number of users in list.\n",
    "    \"\"\"\n",
    "    users = set()\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        key = 'user'\n",
    "        if key in element.attrib:\n",
    "            users.add(element.attrib['user'])\n",
    "    return users\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AndrewBuck',\n",
       " 'John_Nagle',\n",
       " 'pluton_od',\n",
       " 'fproulx',\n",
       " 'anaris328',\n",
       " 'mnorelli',\n",
       " 'Kim Mason',\n",
       " 'OYR',\n",
       " 'bballguy',\n",
       " 'adjuva',\n",
       " 'KaiRo',\n",
       " 'CWHz',\n",
       " 'Sundance',\n",
       " 'Brian@Brea',\n",
       " 'sladen',\n",
       " 'Wim L',\n",
       " 'posdata',\n",
       " 'Davidazus',\n",
       " 'OSMF Redaction Account',\n",
       " 'bielebog',\n",
       " 'aguynamedben',\n",
       " 'RichRico',\n",
       " 'jorge_o',\n",
       " 'osmcrc',\n",
       " 'Colin Jensen',\n",
       " 'Sarr_Cat',\n",
       " 'omgar',\n",
       " 'danrademacher',\n",
       " 'rowers2',\n",
       " 'David Speakman',\n",
       " 'ramyaragupathy',\n",
       " 'DanDaMan123',\n",
       " 'jsnake72',\n",
       " 'nextuniverse',\n",
       " 'LiisiS',\n",
       " 'corvy12',\n",
       " 'Tronikon',\n",
       " 'allandaly',\n",
       " 'knockpenny',\n",
       " 'rgupta0747',\n",
       " 'Manu1400',\n",
       " 'MappedReduced',\n",
       " 'AmanB',\n",
       " 'Cjlanda',\n",
       " 'ezekielf',\n",
       " 'jmtyndall',\n",
       " 'Field Enterprise',\n",
       " 'Karen Coyle',\n",
       " 'BharataHS_sfimport',\n",
       " 'Warren T',\n",
       " 'yurasi',\n",
       " 'Stephen214',\n",
       " 'lenlo',\n",
       " 'Jesus arango',\n",
       " 'dandv',\n",
       " 'codesliced',\n",
       " 'ryandrake',\n",
       " 'Meersbrook',\n",
       " 'David Strauss',\n",
       " 'jdhall',\n",
       " 'California Bear',\n",
       " 'Bay Area Tango',\n",
       " 'dirtysouth',\n",
       " 'Keith Cheveralls',\n",
       " 'Blaxcarab',\n",
       " 'Jothirnadh',\n",
       " 'probabble',\n",
       " 'volki123',\n",
       " 'cumbop',\n",
       " 'edwilcox',\n",
       " 'Alan Vogt',\n",
       " 'InsertUser',\n",
       " 'malcolmh',\n",
       " 'istvanv_telenav',\n",
       " 'happy5214',\n",
       " 'masterleep',\n",
       " 'jacobbraeutigam',\n",
       " 'rainbowhologram',\n",
       " 'ajashton',\n",
       " 'janp2001',\n",
       " 'hostels',\n",
       " 'jspencer24',\n",
       " 'kaisako',\n",
       " 'pizza4days',\n",
       " 'djlb',\n",
       " 'Kristymarie42',\n",
       " 'hfyu',\n",
       " 'SVRK',\n",
       " 'Rodolphe Devillers',\n",
       " 'xxbraunxx',\n",
       " 'HeyoYeyo',\n",
       " 'saikabhi',\n",
       " 'sorchah',\n",
       " 'fermenting',\n",
       " 'beweta',\n",
       " 'Map King',\n",
       " 'hazzardsb',\n",
       " 'BuganiniQ',\n",
       " 'kronick',\n",
       " 'Minh Nguyen',\n",
       " '42429',\n",
       " 'LeTopographeFou',\n",
       " 'askklein',\n",
       " 'sheeter',\n",
       " 'minewman',\n",
       " 'dpaschich',\n",
       " 'baditaflorin',\n",
       " 'Alan',\n",
       " 'James Fahlbusch',\n",
       " 'kdingman',\n",
       " 'jogger333',\n",
       " 'Aleks-Berlin',\n",
       " 'edgehillnet',\n",
       " 'Apo42',\n",
       " 'ansis',\n",
       " 'DKNOTT',\n",
       " u'\\u042e\\u043a\\u0430\\u0442\\u0430\\u043d',\n",
       " 'seanpete',\n",
       " 'rducky26',\n",
       " 'gwicke',\n",
       " 'jotdown',\n",
       " 'oldtopos',\n",
       " 'dufekin',\n",
       " 'sfllaw',\n",
       " 'woodpeck_repair',\n",
       " 'mhenson',\n",
       " 'Spanholz',\n",
       " 'granpueblo',\n",
       " 'xenotropic',\n",
       " 'geodreieck4711',\n",
       " 'Jason Scheirer',\n",
       " 'BerkeleyRedneck',\n",
       " 'FTA',\n",
       " 'AE35',\n",
       " 'Matt Ball',\n",
       " 'Jack J Jia',\n",
       " 'Aaronmap',\n",
       " 'shivanshu',\n",
       " 'xybot',\n",
       " 'jforbess',\n",
       " 'mannykary',\n",
       " 'MiyagiMachine',\n",
       " 'ediyes',\n",
       " 'sundaykofax',\n",
       " 'jotzt',\n",
       " 'nmixter',\n",
       " 'Peter Shank',\n",
       " 'MichaelGSmith',\n",
       " 'BitcoinMaps',\n",
       " '13 digits',\n",
       " 'Melaskia',\n",
       " 'marcik4',\n",
       " 'HeyIdiot',\n",
       " 'bjkosm',\n",
       " 'Bob12345',\n",
       " 'DagM',\n",
       " 'Schierkolk',\n",
       " 'TheMappedAvenger',\n",
       " 'ssjazz',\n",
       " 'kateleslie',\n",
       " 'Thibaut75011',\n",
       " 'nbadg',\n",
       " 'DMaximus',\n",
       " 'NewSon',\n",
       " 'n76',\n",
       " 'uboot',\n",
       " 'amadorschulze92',\n",
       " 'WBSKI',\n",
       " 'Bryce C Nesbitt',\n",
       " 'Dero Bike Racks',\n",
       " 'Bman',\n",
       " 'AntonBryl',\n",
       " 'Popolon',\n",
       " 'dogtired75',\n",
       " 'it_was_catch22',\n",
       " 'Sean Blake',\n",
       " 'Andras Fabian',\n",
       " 'morganvenable',\n",
       " 'Leon K',\n",
       " 'Zoltan Puskas',\n",
       " 'Darin Jensen',\n",
       " 'Harry Cutts',\n",
       " 'Octoferret',\n",
       " 'volker g',\n",
       " 'marschw',\n",
       " 'nikhilprabhakar',\n",
       " 'tenkasi',\n",
       " 'K6RHL',\n",
       " 'matthieun',\n",
       " 'icdawg',\n",
       " 'VrindaM',\n",
       " 'snowey',\n",
       " 'noffle',\n",
       " 'zachlipton',\n",
       " 'lwu',\n",
       " 'warneke7',\n",
       " 'osmmaker',\n",
       " 'MikeN',\n",
       " 'Nova Pierce',\n",
       " 'dkess',\n",
       " 'suspet',\n",
       " 'klyle14',\n",
       " 'animeigo',\n",
       " 'ChrisZontine',\n",
       " 'bheinema',\n",
       " '00crashtest',\n",
       " 'Gym Everywhere',\n",
       " 'ypid',\n",
       " 'abannert',\n",
       " 'rthfer',\n",
       " 'delphiN',\n",
       " 'doug_sfba',\n",
       " 'David Selassie',\n",
       " 'hmyers909',\n",
       " 'Bhojaraj',\n",
       " 'skorasaurus',\n",
       " 'BHSPitMonkey',\n",
       " 'Walkabout44',\n",
       " 'CPlank',\n",
       " 'muziriana',\n",
       " 'eMarcus',\n",
       " 'rougeli',\n",
       " 'mcdmx',\n",
       " 'ruthmaben',\n",
       " 'Teuxe',\n",
       " 'AM909',\n",
       " 'gfysgerm',\n",
       " 'MrRedwood',\n",
       " 'GITNE',\n",
       " 'Xander Kittah',\n",
       " 'Roadrenner',\n",
       " 'CarolynG',\n",
       " 'Zartbitter',\n",
       " 'bhalperin28',\n",
       " 'TimA',\n",
       " 'kphaneuf',\n",
       " 'braveally',\n",
       " 'bravo7265',\n",
       " 'ShopParkStreet',\n",
       " 'GPJ123',\n",
       " 'streeter',\n",
       " 'scai',\n",
       " '4b696d',\n",
       " 'Tom O',\n",
       " 'pnorman_mechanical',\n",
       " 'PH_Mapper',\n",
       " 'Sam Maurer',\n",
       " 'lchow72',\n",
       " 'cdhen12',\n",
       " 'glennon',\n",
       " 'lbacud',\n",
       " 'Wai Yip Tung',\n",
       " 'ethelmermaid',\n",
       " 'Bootprint',\n",
       " 'Shmias',\n",
       " 'cercis14',\n",
       " 'taylorh',\n",
       " 'Seandebasti',\n",
       " 'PeterEastern',\n",
       " 'Jayen A',\n",
       " 'Raymond',\n",
       " 'Dilys',\n",
       " 'Kevin Girard',\n",
       " 'KingDedede',\n",
       " 'dbaron',\n",
       " 'Jeremy Laskar',\n",
       " 'dolceBottle',\n",
       " 'dadsdomo',\n",
       " 'dsherrell',\n",
       " 'SirCxyrtyx',\n",
       " 'mapsru',\n",
       " 'andygol',\n",
       " 'camilleanne',\n",
       " 'Guy Noir',\n",
       " 'sotovm',\n",
       " 'coleman',\n",
       " 'KevinGillette',\n",
       " 'HugoMorales',\n",
       " 'Hayley Drury',\n",
       " 'Andrew Kraut',\n",
       " 'friedbunny',\n",
       " 'srividya_c',\n",
       " 'jrissman',\n",
       " 'Emers',\n",
       " 'dchiles',\n",
       " 'BayMapper',\n",
       " 'xj25vm',\n",
       " 'White_Rabbit',\n",
       " 'wheelmap_visitor',\n",
       " 'johnwilde',\n",
       " 'Michael_SFBA',\n",
       " 'Jchoe126',\n",
       " 'Mike Dvorak',\n",
       " 'IndyMapper',\n",
       " 'robgeb',\n",
       " 'jasonost',\n",
       " 'ingalls',\n",
       " 'burritojustice',\n",
       " 'craftsbury',\n",
       " 'rmikke',\n",
       " 'zoverax',\n",
       " 'upendra_sfimport',\n",
       " 'Max Klein',\n",
       " 'Oski',\n",
       " 'Mgahinga',\n",
       " 'afdreher',\n",
       " 'mjn',\n",
       " 'mueschel',\n",
       " 'Jam Mistry',\n",
       " 'GeorgMap',\n",
       " 'adbrown',\n",
       " 'edisonlow',\n",
       " 'jdmonin',\n",
       " 'Rupert Swarbrick',\n",
       " 'jnmiller',\n",
       " 'Verdy_p',\n",
       " 'Supaplex',\n",
       " 'GigglingMapper',\n",
       " 'chintanmistry',\n",
       " 'rachelahouser',\n",
       " 'maxerickson',\n",
       " 'riordabr',\n",
       " 'Chetan_Gowda',\n",
       " 'oini',\n",
       " 'Ben Discoe',\n",
       " 'vbgt',\n",
       " 'Steve145',\n",
       " 'Janishag',\n",
       " 'Nelson Minar',\n",
       " 'MichaelSteffen',\n",
       " 'amillar',\n",
       " 'PDelahanty',\n",
       " 'davetoo',\n",
       " 'ulrich_',\n",
       " 'Slam Garbage',\n",
       " 'jnstahl',\n",
       " 'stepps00',\n",
       " 'Christine_E',\n",
       " 'wiedmann',\n",
       " 'gnomad83',\n",
       " 'Julian Mundhahs',\n",
       " 'Andrew Ash',\n",
       " 'Michael Webster',\n",
       " 'kingpenguin',\n",
       " 'headwatersolver',\n",
       " 'Matt Heberger',\n",
       " 'Luke317',\n",
       " 'ArgonHunter',\n",
       " 'lyiu',\n",
       " 'sjdunbar',\n",
       " 'boeleman81',\n",
       " 'das-gus',\n",
       " 'RAW',\n",
       " 'daviddengcn',\n",
       " 'Scott13208',\n",
       " 'lbud',\n",
       " 'godzilla94552',\n",
       " 'cynmar',\n",
       " '-MegaByte-',\n",
       " 'theillustratedlife',\n",
       " 'BharataHS_laimport',\n",
       " 'MonoSim',\n",
       " 'erik2d2',\n",
       " 'bamboo2',\n",
       " 'xaric',\n",
       " 'leftcoastnaturalist',\n",
       " 'JeLuF',\n",
       " 'Sarah M',\n",
       " 'dacheatbot',\n",
       " 'catrip11',\n",
       " 'Ashish Vijayaram',\n",
       " 'Ben Osheroff',\n",
       " 'IbolyaTarsoly',\n",
       " 'MindTheGap',\n",
       " 'ecloud',\n",
       " 'eepsmedia',\n",
       " 'wilmette88',\n",
       " 'drabe',\n",
       " 'sfodoug',\n",
       " 'MAPconcierge',\n",
       " 'Joe Hughes',\n",
       " 'Zygmunt Czykieta',\n",
       " 'Roozbeh',\n",
       " 'blackadder',\n",
       " 'UltraNivlac',\n",
       " 'YongYang',\n",
       " 'Kathy Vinson',\n",
       " 'Esperanza36',\n",
       " 'juergenb22',\n",
       " 'mcmm',\n",
       " '-ad-',\n",
       " 'Miles Sanford',\n",
       " 'DanHomerick',\n",
       " 'MaxSem',\n",
       " 'jonesydesign',\n",
       " 'Laurentius',\n",
       " 'sebastic',\n",
       " 'jerememonteau',\n",
       " 'justingrosch',\n",
       " 'Milo',\n",
       " 'neighborhoodsnoop',\n",
       " 'Telecas',\n",
       " 'Barry Parr',\n",
       " 'mdk',\n",
       " 'Mark Koo',\n",
       " 'pripager',\n",
       " 'StellanL',\n",
       " 'atsiatas',\n",
       " 'glisse',\n",
       " 'MrJamesonNeat',\n",
       " 'Andreyhmk',\n",
       " 'GulMaikat',\n",
       " 'luckiegurl37',\n",
       " 'AndrewGG',\n",
       " 'remingtonhotels',\n",
       " u'L\\xe9o Frachet',\n",
       " 'WayneSchlegel',\n",
       " 'davidearl',\n",
       " 'conf88',\n",
       " 'DongyangCA',\n",
       " 'Hiker Dan',\n",
       " 'Jason Harrison',\n",
       " 'amorde',\n",
       " 'lyzid',\n",
       " 'richlv',\n",
       " 'cartographe',\n",
       " 'Yury Yatsynovich',\n",
       " 'hobbesvsboyle',\n",
       " '!link',\n",
       " 'aarp65',\n",
       " 'phut',\n",
       " 'mcheung',\n",
       " 'robert',\n",
       " 'mvexel',\n",
       " 'ReinerMeyer',\n",
       " 'Jenny Redo',\n",
       " 'SanderI44',\n",
       " 'YenTheFirst',\n",
       " 'T99',\n",
       " 'immle',\n",
       " 'Basstoelpel',\n",
       " 'bigwebguy',\n",
       " 'MartinJW',\n",
       " 'Dirbam',\n",
       " 'Jsolans',\n",
       " 'Mappy',\n",
       " 'Joe Freedman',\n",
       " 'sycamore_island',\n",
       " 'migurski',\n",
       " 'McKinley D',\n",
       " 'jeremiak',\n",
       " 'jkfischer',\n",
       " 'Sendavo',\n",
       " 'thecrazyknitter',\n",
       " 'Rie Wakab',\n",
       " 'standtoandfore',\n",
       " 'sirmmo',\n",
       " 'Thomas Levine',\n",
       " 'Reno Editor',\n",
       " 'Mapper1',\n",
       " 'Heinz_V',\n",
       " 'mxndrwgrdnr',\n",
       " 'Byung Kyu Park',\n",
       " 'Ken Oates',\n",
       " 'Z440',\n",
       " 'lowlamaps',\n",
       " 'BharataHS',\n",
       " 'Gone',\n",
       " 'derricknehrenberg',\n",
       " 'bagnolais',\n",
       " 'SpannedObjurgations',\n",
       " 'MrMystery314',\n",
       " 'Kimberlyn',\n",
       " 'landesb',\n",
       " 'ericeatsbrains',\n",
       " 'jgkamat',\n",
       " 'c tito young',\n",
       " 'Imp_GL',\n",
       " 'Clubby72',\n",
       " 'zel105',\n",
       " 'caguilerale',\n",
       " 'brunosan',\n",
       " 'Morgan Herlocker',\n",
       " 'jdeheer',\n",
       " 'Latze',\n",
       " 'smlevine',\n",
       " 'Mapsack',\n",
       " 'DF99',\n",
       " 'Travl',\n",
       " 'Thi Duong',\n",
       " 'emejim',\n",
       " 'Will Payne',\n",
       " 'dnomadb',\n",
       " 'powell789',\n",
       " 'cityeditor1000',\n",
       " 'Caboosey',\n",
       " 'dbaupp',\n",
       " 'ELadner',\n",
       " 'BLUG_Julien',\n",
       " 'txhenry',\n",
       " 'steverumizen',\n",
       " 'Dieter Schmeer',\n",
       " 'mbiker',\n",
       " 'pratikyadav',\n",
       " 'adammann930',\n",
       " 'Michael H Arnold',\n",
       " 'Iq19zero',\n",
       " 'Claudev8',\n",
       " '@kevin_bullock',\n",
       " 'jfire',\n",
       " 'parkerw13',\n",
       " 'emilyann5',\n",
       " 'esuor',\n",
       " 'taifunbrowser',\n",
       " 'nikhil_imports',\n",
       " 'dmitrig01',\n",
       " 'GruberN92',\n",
       " 'ipsofacto',\n",
       " 'paulinespizza',\n",
       " 'LightPoet',\n",
       " 'Norbert Kiesel',\n",
       " 'SocMapper',\n",
       " 'trosm',\n",
       " 'Ryan Scott',\n",
       " 'JoanL',\n",
       " 'anonymoussparrow',\n",
       " 'saikabhi_sfimport',\n",
       " 'M_Aloisius',\n",
       " 'theburg',\n",
       " 'sabas88',\n",
       " 'npdoty',\n",
       " 'frallain',\n",
       " 'Probably',\n",
       " 'ALowe13',\n",
       " 'birdhome42',\n",
       " 'bugplanet',\n",
       " 'AUilocks',\n",
       " 'Marion Barry',\n",
       " 'cmdd',\n",
       " 'sayreot',\n",
       " 'Snackdaddy',\n",
       " 'thomas_kregelin',\n",
       " 'mestergaard',\n",
       " 'Tiara Richard',\n",
       " 'pete404',\n",
       " 'jthuannguyen',\n",
       " 'dschulle',\n",
       " 'chri24',\n",
       " 'Derick Rethans',\n",
       " 'J Bolivar',\n",
       " 'phoenix23',\n",
       " 'Julio_Costa_Zambelli',\n",
       " 'Jfact0ry',\n",
       " 'wallclimber21',\n",
       " 'andeux',\n",
       " 'Eric Fischer',\n",
       " 'SeanMichetti',\n",
       " 'Nathan K',\n",
       " 'fbgeo',\n",
       " 'Steven1313',\n",
       " 'SFBay1234',\n",
       " 'RaffyO',\n",
       " 'RengarOP',\n",
       " 'KindredCoda',\n",
       " 'saikofish',\n",
       " 'Brendan6223',\n",
       " 'Audey Seawright',\n",
       " 'AlaskaDave',\n",
       " 'DennisL',\n",
       " 'RationalTangle',\n",
       " 'vonvonvon',\n",
       " 'DaveHansenTiger',\n",
       " 'jcdoll',\n",
       " 'spift',\n",
       " 'markmisener',\n",
       " 'tmalloy',\n",
       " 'Richard',\n",
       " 'oormilavinod',\n",
       " 'jrpepper',\n",
       " 'Hbomb1977',\n",
       " 'PA94',\n",
       " 'CitymapperHQ',\n",
       " 'bnewey',\n",
       " 'nickvet419',\n",
       " 'thetornado76',\n",
       " 'seattlefyi',\n",
       " 'thevirginian',\n",
       " 'Rick Free',\n",
       " 'pescorosso',\n",
       " 'DenisCarriere',\n",
       " 'lcmortensen',\n",
       " 'johnnyishellarad',\n",
       " 'jinalfoflia',\n",
       " 'savannahsmith',\n",
       " 'Skybunny',\n",
       " 'PlaneMad',\n",
       " 'Gizmot',\n",
       " 'magnunor',\n",
       " 'bron',\n",
       " 'tsangkenneth',\n",
       " 'Darragh_M',\n",
       " 'HolgerJeromin',\n",
       " 'BRAVO-SOC128-2013',\n",
       " 'WimM',\n",
       " 'Scottieb3',\n",
       " 'gaku',\n",
       " 'Ahlzen',\n",
       " 'Alex-7',\n",
       " 'Peter Black',\n",
       " 'manoharuss',\n",
       " 'Ropino',\n",
       " 'Ludmila Gladkova',\n",
       " 'dhalli1',\n",
       " 'guri007',\n",
       " 'DaveHunt',\n",
       " 'kouker',\n",
       " 'Charles_Smothers',\n",
       " 'emeryradio',\n",
       " 'Aaron Lidman',\n",
       " 'rolandg',\n",
       " 'flydood',\n",
       " 'thatmm',\n",
       " 'Ch0mCh0m',\n",
       " 'baversjo',\n",
       " 'lucioperca',\n",
       " 'thegodfather94',\n",
       " 'theckman',\n",
       " 'xaberoth',\n",
       " 'axelbaker',\n",
       " 'TorhamZed',\n",
       " 'ryanderso',\n",
       " 'aihardin',\n",
       " 'Joel Franusic',\n",
       " 'rene78',\n",
       " 'YannC',\n",
       " \"Ethan O'Connor\",\n",
       " 'ridixcr',\n",
       " 'fennecfoxen',\n",
       " 'senordanimal',\n",
       " 'Speight',\n",
       " 'Chris Bolt 136',\n",
       " 'Teslawire',\n",
       " 'Omnific',\n",
       " 'angier1212',\n",
       " 'mllefoo',\n",
       " 'aude',\n",
       " 'MikeChuck',\n",
       " 'Zer',\n",
       " 'garmercman',\n",
       " 'TJ Crowder',\n",
       " 'jknewl',\n",
       " 'hichmont',\n",
       " 'pankdm',\n",
       " 'mrspie',\n",
       " 'g246020',\n",
       " 'forkbender',\n",
       " 'dholbert',\n",
       " 'ypao',\n",
       " 'Keykeys',\n",
       " 'dspring',\n",
       " 'mikegfi',\n",
       " 'aleksandrvladimirskiy',\n",
       " 'adamos',\n",
       " 'cyeager89',\n",
       " 'Davis Kitchel',\n",
       " 'homeslice60148',\n",
       " 'bpace1994',\n",
       " 'mbuege',\n",
       " 'giantloops',\n",
       " 'dennisclark',\n",
       " 'pnorman',\n",
       " 'Devin Swisher',\n",
       " 'Dave Parker',\n",
       " 'Arrangy',\n",
       " 'cablemotion',\n",
       " 'digital0533',\n",
       " 'AlistairA',\n",
       " 'JM7323',\n",
       " 'pkm',\n",
       " 'karitotp',\n",
       " 'Vlad',\n",
       " 'BK_man',\n",
       " 'Chris Lawrence',\n",
       " 'eavrdfsdcwad',\n",
       " 'Stephanie May',\n",
       " 'DrSilentMoon',\n",
       " 'upendrakarukonda',\n",
       " 'rabbitface',\n",
       " 'Andy88',\n",
       " 'Alecs01',\n",
       " 'barbearian',\n",
       " 'FactChecker',\n",
       " 'Tom Brown',\n",
       " 'ryanbranciforte',\n",
       " 'Paul Everett',\n",
       " 'tedski',\n",
       " 'MerryMunchie',\n",
       " 'shitonfools',\n",
       " 'jimhu',\n",
       " 'ElliottPlack',\n",
       " 'Joseph Churchill',\n",
       " 'Tomo666',\n",
       " 'nereocystis',\n",
       " 'Ballard OpenStreetMap',\n",
       " 'Peter A',\n",
       " 'bespoke89',\n",
       " 'Tom Morris',\n",
       " 'enveloper',\n",
       " 'IanH',\n",
       " 'howardahn',\n",
       " 'airplane121',\n",
       " 'tbm',\n",
       " 'AllieJanoch',\n",
       " 'NWR6',\n",
       " 'mojodna',\n",
       " 'DHus',\n",
       " 'claysmalley',\n",
       " 'njaard',\n",
       " 'Jenn Kaplan',\n",
       " 'Fishytodd',\n",
       " 'LXT',\n",
       " 'Adam Alpern',\n",
       " 'Taroc',\n",
       " 'olikami',\n",
       " 'FilG',\n",
       " 'bmhr',\n",
       " 'Serguei Trouchelle',\n",
       " 'Mateusz Konieczny',\n",
       " 'CLindh',\n",
       " 'emancdf',\n",
       " 'elharaty',\n",
       " 'sea2sky',\n",
       " 'Matthew Zehnder',\n",
       " 'dannykath',\n",
       " 'sethoscope',\n",
       " 'cdrini',\n",
       " 'philsevansdesigh',\n",
       " 'AndrewSnow',\n",
       " '0101010',\n",
       " 'Joel S',\n",
       " 'DougPeterson',\n",
       " 'Cosa Maria',\n",
       " 'piligab',\n",
       " 'Dave Guarino',\n",
       " 'Eric Bateman',\n",
       " 'nithinkamath',\n",
       " 'Ryan Maki',\n",
       " 'Nick L',\n",
       " 'rickmastfan67',\n",
       " 'AndreasPr',\n",
       " 'swimdb',\n",
       " 'glotto',\n",
       " 'Jesper Jurcenoks',\n",
       " 'vkungys',\n",
       " 'szymanska',\n",
       " 'Alex Wilton',\n",
       " 'mneko',\n",
       " 'Nick Burrett',\n",
       " 'duxxa',\n",
       " 'Ken Johnson 2',\n",
       " 'Timothy Smith',\n",
       " 'Stheeve',\n",
       " 'travelingkev',\n",
       " 'Dan Ryan',\n",
       " 'drewda',\n",
       " 'joesipad',\n",
       " 'egavril',\n",
       " 'countableSet',\n",
       " 'jlev',\n",
       " 'velonautian',\n",
       " 'olanaso',\n",
       " 'ashleyannmathew',\n",
       " 'declerck',\n",
       " 'Nemo_bis',\n",
       " 'Travel Robot 1000X',\n",
       " 'Sebastien R',\n",
       " 'Fa7C0N',\n",
       " 'jraller',\n",
       " 'KaylanHager',\n",
       " 'Daniel Tai',\n",
       " 'Nimbalo',\n",
       " 'isabellekh',\n",
       " 'pdgoodman',\n",
       " 'DLichti',\n",
       " 'elsaxo',\n",
       " 'jazzshuffle',\n",
       " 'Data411',\n",
       " 'osmaung',\n",
       " 'thepoobs',\n",
       " 'liuweixyr',\n",
       " 'lcwong',\n",
       " 'Rudloff',\n",
       " 'teodorab_telenav',\n",
       " 'spoon!',\n",
       " 'mapmeld',\n",
       " 'someToast',\n",
       " 'Naomi_Good',\n",
       " 'ORDG',\n",
       " 'groove',\n",
       " 'mpmckenna8',\n",
       " 'Nate_Wessel',\n",
       " 'SF-Brian',\n",
       " 'Jason Lee',\n",
       " 'Eseqko',\n",
       " 'dmgroom_ct',\n",
       " 'Steve',\n",
       " 'ki6uoc',\n",
       " 'Iowa Kid',\n",
       " 'Kenji Yamada',\n",
       " 'Rory Nealon',\n",
       " 'ddtuga',\n",
       " 'cdavila',\n",
       " 'Natfoot',\n",
       " 'vvoovv',\n",
       " 'brentengust',\n",
       " 'Gum4eg',\n",
       " 'PeterPablo',\n",
       " 'CaseyFrost',\n",
       " 'DrSeamons',\n",
       " 'mglasser',\n",
       " 'msdess',\n",
       " 'jmadeline',\n",
       " 'Rupert Clayton',\n",
       " 'acjacobson',\n",
       " 'sissou',\n",
       " 'knotthere',\n",
       " 'Diegophil',\n",
       " 'Daniel Specht',\n",
       " 'Jesse Phillips',\n",
       " 'fogfish',\n",
       " 'fiveisalive',\n",
       " 'jaylina',\n",
       " 'cylexx',\n",
       " 'oakland',\n",
       " 'ry-pi',\n",
       " 'Sunfishtommy',\n",
       " 'fhmh',\n",
       " 'Srittau',\n",
       " 'Crabkilla',\n",
       " 'encleadus',\n",
       " 'Alx303',\n",
       " 'rkuris',\n",
       " 'mtang2',\n",
       " 'balrog-kun',\n",
       " 'Christian Elsen',\n",
       " 'wegavision',\n",
       " 'revent',\n",
       " 'hno2',\n",
       " 'rstoeber',\n",
       " 'NLgis',\n",
       " 'MenloJim',\n",
       " 'alpinekid',\n",
       " 'ramyaragupathy_sfimport',\n",
       " 'eddietejeda',\n",
       " 'Paul Johnson',\n",
       " 'bisho',\n",
       " 'RMaryan',\n",
       " 'mobilenavigator',\n",
       " 'riiga',\n",
       " 'Josh Hill',\n",
       " 'tahongawaka',\n",
       " 'Porphyrion',\n",
       " 'cwindsor',\n",
       " 'Luis36995',\n",
       " 'mmckinst',\n",
       " 'Adityo',\n",
       " 'bahnpirat',\n",
       " 'jveeb',\n",
       " 'liammc2000',\n",
       " 'AGWA',\n",
       " 'JessAk71',\n",
       " 'rainygarden',\n",
       " 'jlawrence',\n",
       " 'Isabel Wang',\n",
       " 'calfarome',\n",
       " 'colindt',\n",
       " 'blackest_knight',\n",
       " 'meghan_oconnell',\n",
       " 'Vazhnov Alexey',\n",
       " 'jtbrown1806',\n",
       " 'Joey Shemuel',\n",
       " 'zacmccormick',\n",
       " 'Daniel JRDV',\n",
       " 'David Muir Sharnoff',\n",
       " 'domob',\n",
       " 'VMukhtarov',\n",
       " 'Alexander Avtanski',\n",
       " 'Andy Allan',\n",
       " 'P Dill',\n",
       " 'Fan Kong',\n",
       " 'Cherry24',\n",
       " 'Broadwell',\n",
       " 'zoeplankton',\n",
       " 'n_LH',\n",
       " 'dvjohnston',\n",
       " 'zoharby',\n",
       " 'theangrytomato',\n",
       " 'zerodameaon',\n",
       " 'Cato_d_Ae',\n",
       " 'kmarseglia',\n",
       " 'Aric',\n",
       " 'CSE_Club',\n",
       " 'Theo D O Lite',\n",
       " 'hofoen',\n",
       " 'Robowolfer',\n",
       " 'pinakographos',\n",
       " 'stkashani',\n",
       " 'bdon_import',\n",
       " 'SJFriedl',\n",
       " 'Michael Ageno',\n",
       " 'k1wi',\n",
       " 'account_deleted_1011',\n",
       " 'woodpeck_fixbot',\n",
       " 'WorldWatchOne',\n",
       " 'smcg',\n",
       " 'Rudolf Mayer',\n",
       " 'Mari G',\n",
       " 'marthaleena',\n",
       " 'Vanuan',\n",
       " 'thegpshiker',\n",
       " 'diskus69',\n",
       " 'Simon Willison2',\n",
       " 'chetan-sfimport',\n",
       " 'sctrojan79',\n",
       " 'pmeehan',\n",
       " 'florinbadita_telenav',\n",
       " 'erjiang',\n",
       " 'parrishioner',\n",
       " 'Drew1sh',\n",
       " 'achims311',\n",
       " 'pollardld',\n",
       " 'Martin Atkins',\n",
       " 'samely',\n",
       " 'Jason Roy',\n",
       " 'trasa_gd',\n",
       " 'highbuilder',\n",
       " 'surfearth',\n",
       " 'warrenwong',\n",
       " 'charlieslin',\n",
       " 'Orblivion',\n",
       " 'mtravlos',\n",
       " 'jz831',\n",
       " 'Bluecow',\n",
       " 'cartobandit',\n",
       " 'kingrobert',\n",
       " 'Erik Lundin',\n",
       " 'ERSV',\n",
       " 'eric22',\n",
       " 'mike140',\n",
       " 'Alps Pierrat',\n",
       " 'aguiles',\n",
       " 'invliD',\n",
       " 'Nikkitangs',\n",
       " 'Helmchen42',\n",
       " 'Sasharef',\n",
       " 'Donteatmeyet',\n",
       " 'Mr0grog',\n",
       " 'Claumires',\n",
       " 'zhurai',\n",
       " 'NAParish',\n",
       " 'shvrm',\n",
       " 'denimboy',\n",
       " 'Jonathan ZHAO',\n",
       " 'jessyess',\n",
       " 'BigD65',\n",
       " 'SamHiatt',\n",
       " 'jeff2',\n",
       " 'Andre68',\n",
       " 'KayaknJim',\n",
       " 'GeoTomDe',\n",
       " 'soundtcr',\n",
       " 'bobnewey',\n",
       " 'flockfinder',\n",
       " 'rachelm',\n",
       " 'HHilton',\n",
       " 'Josef73',\n",
       " 'dret',\n",
       " 'chadlawlis',\n",
       " 'KR-KRKR-KR',\n",
       " 'beddy',\n",
       " 'talldave',\n",
       " 'db48x',\n",
       " 'sys49152',\n",
       " 'ub91',\n",
       " 'JohnSmith',\n",
       " 'jgoguen',\n",
       " 'kathleenlu',\n",
       " 'mavl',\n",
       " 'Guy Bruneau',\n",
       " 'edenh-mapbox',\n",
       " 'Olivier Carbonneau',\n",
       " 'John Gilmore',\n",
       " 'theojapa',\n",
       " 'wyleyr',\n",
       " 'polarwishes',\n",
       " 'Gregory Arenius',\n",
       " 'Amanda Seaman L Ac',\n",
       " 'dharber',\n",
       " 'Dr Kludge',\n",
       " 'Gallinger',\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_map('sample_sf.osm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of people.  Probably safe to assume they live or have lived in San Francisco.  I've only been to SF once in my life as a wee little tike, so I don't remember much.  Maybe I should hit one of these people up and guide me around town.  (^_^)  \n",
    "___\n",
    "### Auditing Street Names:\n",
    "There are many ways to label addresses; Avenue could be Ave, ave, ave or heck maybe even a full on typo like avenie.  Lets check.  Game plan here is to create a cross check of our street values with a list of expected labels.  If they don't then we'll want to correct them so our data is **consistent**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OSMFILE = \"sample_sf.osm\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "num_line_street_re = re.compile(r'\\d0?(st|nd|rd|th|)\\s(Line)$', re.IGNORECASE) # Spell lines ten and under\n",
    "nth_re = re.compile(r'\\d\\d?(st|nd|rd|th|)', re.IGNORECASE)\n",
    "nesw_re = re.compile(r'\\s(North|East|South|West)$')\n",
    "\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\", \"Circle\", \"Crescent\", \"Gate\", \"Terrace\", \"Grove\", \"Way\"]\n",
    "\n",
    "mapping = { \n",
    "            \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"st\": \"Street\",\n",
    "            \"STREET\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"Ave,\": \"Avenue\",\n",
    "            \"ave\": \"Avenue\",\n",
    "            \"ave.\": \"Avenue\",\n",
    "            \"ave,\": \"Avenue\",\n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Blvd.\": \"Boulevard\",\n",
    "            \"Blvd,\": \"Boulevard\",\n",
    "            \"blvd\": \"Boulevard\",\n",
    "            \"blvd.\": \"Boulevard\",\n",
    "            \"blvd,\": \"Boulevard\",\n",
    "            \"Ehs\": \"EHS\",\n",
    "            \"Trl\": \"Trail\",\n",
    "            \"Cir\": \"Circle\",\n",
    "            \"Cir.\": \"Circle\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Ct.\": \"Court\",\n",
    "            \"Ctr\": \"Center\",\n",
    "            \"Ctr.\": \"Center\",\n",
    "            \"Ctr,\": \"Center\",\n",
    "            \"ctr\": \"Center\",\n",
    "            \"ctr.\": \"Center\",\n",
    "            \"ctr,\": \"Center\",\n",
    "            \"Crt\": \"Court\",\n",
    "            \"Crt.\": \"Court\",\n",
    "            \"By-pass\": \"Bypass\",\n",
    "            \"N.\": \"North\",\n",
    "            \"N\": \"North\",\n",
    "            \"E.\": \"East\",\n",
    "            \"E\": \"East\",\n",
    "            \"S.\": \"South\",\n",
    "            \"S\": \"South\",\n",
    "            \"W.\": \"West\",\n",
    "            \"W\": \"West\"\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "\n",
    "def audit(osmfile):\n",
    "    \"\"\"\n",
    "    Interate through the OSM file and return a dictionary containing street labels as keys\n",
    "    and values of street names found with stated street labels.\n",
    "    \"\"\"\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "def update_name(name, mapping): # name is the actual string of the street, ex:  Keary st\n",
    "    \"\"\"(str, dict) -> str\n",
    "    Check if ending chars of name contain an abbreviation, \n",
    "    if True, return updated name replacing abbreviation with matching value in mapping dictionary\n",
    "    \"\"\"\n",
    "    n = street_type_re.search(name)  #this will check to see if there is an abbreviation at the end of name\n",
    "    if n:  #if True, that is if there is indeed an identified abbreviation...\n",
    "        street_type = n.group()  # here we will group or segment that abbreviation using group()\n",
    "        if street_type in mapping: # if that abbreviation is found in our manual mapping\n",
    "            name = name[:-len(street_type)] + mapping[street_type]  # it will swap it out here with the value in our dict mapping.\n",
    "    return name\n",
    "\n",
    "def update_names(audited_file):\n",
    "    for street_type, ways in audited_file.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            print name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audited_streets_sf = audit(OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we see the dictionary contain the street labels and the street names that contain them.  There are a handful of streets that have their labels abbreviated.  To have our data be uniform, I'm going to opt we use the full name instead of abbreviations, ex:  Blvd -> Boulevard.  This is going to take a bit of extra work but I'll have to review each line of output from above and update my mapping to make sure I catch all street abbreviations I want to be updated.  But once I do, I'll iterate through the OSM file and update the street names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bridgeway => Bridgeway\n",
      "10675 => 10675\n",
      "Fort Mason => Fort Mason\n",
      "foothill blvd => foothill Boulevard\n",
      "Kearny st => Kearny Street\n",
      "Alameda => Alameda\n",
      "Bayshore Highway => Bayshore Highway\n",
      "Great Highway => Great Highway\n",
      "Westlake Center => Westlake Center\n",
      "El Camino Real => El Camino Real\n",
      "Cabrillo Highway North => Cabrillo Highway North\n",
      "South Park => South Park\n",
      "The Embarcadero => The Embarcadero\n",
      "Wildwood Gardens => Wildwood Gardens\n",
      "Indian Rock Path => Indian Rock Path\n",
      "Monte Verde Dr => Monte Verde Drive\n",
      "Avenue Del Ora => Avenue Del Ora\n",
      "Oakridge => Oakridge\n",
      "Avenue D => Avenue D\n",
      "Marina Boulevard Building D => Marina Boulevard Building D\n",
      "Serramonte Ctr => Serramonte Center\n",
      "United Nations Plaza => United Nations Plaza\n",
      "El Cerrito Plaza => El Cerrito Plaza\n",
      "24th St => 24th Street\n",
      "New Montgomery St => New Montgomery Street\n",
      "Market St => Market Street\n",
      "Valencia St => Valencia Street\n",
      "San Francisco Internation Airport => San Francisco Internation Airport\n",
      "Broadway => Broadway\n",
      "Woodside Road, Suite 100 => Woodside Road, Suite 100\n",
      "Nw Quad I-280 / Sr 35 Ic @ Jct Hayne Rd, Golf Course Dr, Skyline Blvd, => Nw Quad I-280 / Sr 35 Ic @ Jct Hayne Rd, Golf Course Dr, Skyline Boulevard\n",
      "La Casa Via => La Casa Via\n",
      "August Alley => August Alley\n",
      "Hodges Alley => Hodges Alley\n",
      "Rose Walk => Rose Walk\n",
      "Terrace Walk => Terrace Walk\n",
      "Tehama Ave => Tehama Avenue\n",
      "Alameda De Las => Alameda De Las\n"
     ]
    }
   ],
   "source": [
    "update_names(audited_streets_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Auditing Zipcodes:\n",
    "Again, doing due-dilligence it wouldn't hurt to check the zip codes for any potential errors.  One thing I learned doing this is that San Francisco's zipcodes all begin in '94'.  And fortunately we can use the code we used to audit the street addresses, albeit with slight variations specific to zipcodes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def audit_zipcode(invalid_zipcodes, zipcode):\n",
    "    twoDigits = zipcode[0:2]\n",
    "    \n",
    "    if twoDigits != 94 or not twoDigits.isdigit():\n",
    "        invalid_zipcodes[twoDigits].add(zipcode)\n",
    "        \n",
    "def is_zipcode(elem):\n",
    "    return (elem.attrib['k'] == \"addr:postcode\")\n",
    "\n",
    "def audit_zip(osmfile):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    invalid_zipcodes = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zipcode(tag):\n",
    "                    audit_zipcode(invalid_zipcodes,tag.attrib['v'])\n",
    "\n",
    "    return invalid_zipcodes\n",
    "\n",
    "audited_zipcode_sf = audit_zip(OSMFILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the audited zipcodes, we see for the most part they are all five digit zipcodes starting with 94.  There are a few with extended zipcodes and one with a zipcode for 95498, which isn't San Francisco.  Another had 'CA' mistakenly placed as a zipcode number.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(set,\n",
       "            {'94': {'94002',\n",
       "              '94010',\n",
       "              '94014',\n",
       "              '94015',\n",
       "              '94019',\n",
       "              '94025',\n",
       "              '94038',\n",
       "              '94044',\n",
       "              '94061',\n",
       "              '94063',\n",
       "              '94065',\n",
       "              '94066',\n",
       "              '94070',\n",
       "              '94080',\n",
       "              '94102',\n",
       "              '94103',\n",
       "              '94104',\n",
       "              '94105',\n",
       "              '94107',\n",
       "              '94108',\n",
       "              '94109',\n",
       "              '94110',\n",
       "              '94111',\n",
       "              '94112',\n",
       "              '94113',\n",
       "              '94114',\n",
       "              '94115',\n",
       "              '94116',\n",
       "              '94117',\n",
       "              '94118',\n",
       "              '94118-1316',\n",
       "              '94121',\n",
       "              '94122',\n",
       "              '94122-1515',\n",
       "              '94123',\n",
       "              '94124',\n",
       "              '94127',\n",
       "              '94128',\n",
       "              '94129',\n",
       "              '94131',\n",
       "              '94132',\n",
       "              '94133',\n",
       "              '94134',\n",
       "              '94303',\n",
       "              '94401',\n",
       "              '94402',\n",
       "              '94403',\n",
       "              '94404',\n",
       "              '94501',\n",
       "              '94507',\n",
       "              '94523',\n",
       "              '94530',\n",
       "              '94541',\n",
       "              '94546',\n",
       "              '94549',\n",
       "              '94555',\n",
       "              '94556',\n",
       "              '94577',\n",
       "              '94578',\n",
       "              '94579',\n",
       "              '94580',\n",
       "              '94587',\n",
       "              '94595',\n",
       "              '94596',\n",
       "              '94597',\n",
       "              '94598',\n",
       "              '94601',\n",
       "              '94602',\n",
       "              '94605',\n",
       "              '94606',\n",
       "              '94607',\n",
       "              '94608',\n",
       "              '94610',\n",
       "              '94611',\n",
       "              '94612',\n",
       "              '94613',\n",
       "              '94618',\n",
       "              '94702',\n",
       "              '94703',\n",
       "              '94704',\n",
       "              '94705',\n",
       "              '94706',\n",
       "              '94707',\n",
       "              '94708',\n",
       "              '94709',\n",
       "              '94710',\n",
       "              '94804',\n",
       "              '94805',\n",
       "              '94901',\n",
       "              '94941',\n",
       "              '94965'},\n",
       "             '95': {'95498'},\n",
       "             'CA': {'CA'}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audited_zipcode_sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_zip(zipcode):\n",
    "    \"\"\"(str) -> str\n",
    "    Return 5 digit zipcodes.\n",
    "    \"\"\"\n",
    "    zipChar = re.findall('[a-zA-Z]*', zipcode)\n",
    "    if zipChar:\n",
    "        zipChar = zipChar[0]\n",
    "    zipChar.strip()\n",
    "    if zipChar == \"CA\":\n",
    "        updateZip = re.findall(r'\\d+', zipcode)\n",
    "        if updateZip:\n",
    "            return (re.findall(r'\\d+', zipcode))[0]\n",
    "    else:\n",
    "        return (re.findall(r'\\d+', zipcode))[0]\n",
    "\n",
    "def update_zips(datafile):\n",
    "    for street_type, ways in audited_zipcode_sf.iteritems():\n",
    "        for name in ways:\n",
    "            better_name = update_zip(name)\n",
    "            print name, \"=>\", better_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CA => None\n",
      "95498 => 95498\n",
      "94404 => 94404\n",
      "94401 => 94401\n",
      "94402 => 94402\n",
      "94403 => 94403\n",
      "94118 => 94118\n",
      "94611 => 94611\n",
      "94610 => 94610\n",
      "94613 => 94613\n",
      "94612 => 94612\n",
      "94112 => 94112\n",
      "94113 => 94113\n",
      "94110 => 94110\n",
      "94111 => 94111\n",
      "94116 => 94116\n",
      "94618 => 94618\n",
      "94114 => 94114\n",
      "94115 => 94115\n",
      "94577 => 94577\n",
      "94578 => 94578\n",
      "94579 => 94579\n",
      "94124 => 94124\n",
      "94123 => 94123\n",
      "94122 => 94122\n",
      "94121 => 94121\n",
      "94129 => 94129\n",
      "94128 => 94128\n",
      "94606 => 94606\n",
      "94607 => 94607\n",
      "94605 => 94605\n",
      "94602 => 94602\n",
      "94601 => 94601\n",
      "94044 => 94044\n",
      "94804 => 94804\n",
      "94805 => 94805\n",
      "94608 => 94608\n",
      "94965 => 94965\n",
      "94010 => 94010\n",
      "94131 => 94131\n",
      "94132 => 94132\n",
      "94133 => 94133\n",
      "94134 => 94134\n",
      "94038 => 94038\n",
      "94507 => 94507\n",
      "94501 => 94501\n",
      "94587 => 94587\n",
      "94580 => 94580\n",
      "94303 => 94303\n",
      "94901 => 94901\n",
      "94025 => 94025\n",
      "94596 => 94596\n",
      "94597 => 94597\n",
      "94595 => 94595\n",
      "94598 => 94598\n",
      "94117 => 94117\n",
      "94015 => 94015\n",
      "94014 => 94014\n",
      "94118-1316 => 94118\n",
      "94019 => 94019\n",
      "94523 => 94523\n",
      "94109 => 94109\n",
      "94080 => 94080\n",
      "94002 => 94002\n",
      "94530 => 94530\n",
      "94708 => 94708\n",
      "94541 => 94541\n",
      "94549 => 94549\n",
      "94122-1515 => 94122\n",
      "94709 => 94709\n",
      "94546 => 94546\n",
      "94707 => 94707\n",
      "94706 => 94706\n",
      "94705 => 94705\n",
      "94704 => 94704\n",
      "94703 => 94703\n",
      "94702 => 94702\n",
      "94070 => 94070\n",
      "94127 => 94127\n",
      "94556 => 94556\n",
      "94555 => 94555\n",
      "94710 => 94710\n",
      "94066 => 94066\n",
      "94108 => 94108\n",
      "94065 => 94065\n",
      "94063 => 94063\n",
      "94061 => 94061\n",
      "94103 => 94103\n",
      "94102 => 94102\n",
      "94105 => 94105\n",
      "94104 => 94104\n",
      "94107 => 94107\n",
      "94941 => 94941\n"
     ]
    }
   ],
   "source": [
    "update_zips(audited_zipcode_sf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Preparing, Loading Data into SQL Database:\n",
    "\n",
    "Now that we've audited and cleaned our data, the next step is to prepare our data into .csv format that will allow it to be imported into a SQL database for further analysis.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OSM_PATH = \"sample_sf.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "def load_new_tag(element, secondary, default_tag_type):\n",
    "    \"\"\"\n",
    "    Load a new tag dict to go into the list of dicts for way_tags, node_tags\n",
    "    \"\"\"\n",
    "    new = {}\n",
    "    new['id'] = element.attrib['id']\n",
    "    if \":\" not in secondary.attrib['k']:\n",
    "        new['key'] = secondary.attrib['k']\n",
    "        new['type'] = default_tag_type\n",
    "\n",
    "    else:\n",
    "        post_colon = secondary.attrib['k'].index(\":\") + 1\n",
    "        new['key'] = secondary.attrib['k'][post_colon:]\n",
    "        new['type'] = secondary.attrib['k'][:post_colon - 1]\n",
    "    new['value'] = secondary.attrib['v']\n",
    "\n",
    "    print \"!23123\"\n",
    "    print secondary.attrib['v']\n",
    "    print\"!2312\"\n",
    "    return new\n",
    "\n",
    "def shape_element(element, node_attr_fields = NODE_FIELDS, way_attr_fields = WAY_FIELDS,\n",
    "                  problem_chars = PROBLEMCHARS, default_tag_type = 'regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        for node in NODE_FIELDS:\n",
    "            node_attribs[node] = element.attrib[node]\n",
    "        for child in element:\n",
    "            tag = {}\n",
    "            if PROBLEMCHARS.search(child.attrib[\"k\"]):\n",
    "                continue\n",
    "        \n",
    "            elif LOWER_COLON.search(child.attrib[\"k\"]):\n",
    "                tag_type = child.attrib[\"k\"].split(':',1)[0]\n",
    "                tag_key = child.attrib[\"k\"].split(':',1)[1]\n",
    "                tag[\"key\"] = tag_key\n",
    "                if tag_type:\n",
    "                    tag[\"type\"] = tag_type\n",
    "                else:\n",
    "                    tag[\"type\"] = 'regular'\n",
    "            \n",
    "                tag[\"id\"] = element.attrib[\"id\"]\n",
    "                tag[\"value\"] = child.attrib[\"v\"]\n",
    "            else:\n",
    "                tag[\"value\"] = child.attrib[\"v\"]\n",
    "                tag[\"key\"] = child.attrib[\"k\"]\n",
    "                tag[\"type\"] = \"regular\"\n",
    "                tag[\"id\"] = element.attrib[\"id\"]\n",
    "            if tag:\n",
    "                tags.append(tag)\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        for way in WAY_FIELDS:\n",
    "            way_attribs[way] = element.attrib[way]\n",
    "        for child in element:\n",
    "            nd = {}\n",
    "            tag = {}\n",
    "            if child.tag == 'tag':\n",
    "                if PROBLEMCHARS.search(child.attrib[\"k\"]):\n",
    "                    continue\n",
    "                elif LOWER_COLON.search(child.attrib[\"k\"]):\n",
    "                    tag_type = child.attrib[\"k\"].split(':',1)[0]\n",
    "                    tag_key = child.attrib[\"k\"].split(':',1)[1]\n",
    "                    tag[\"key\"] = tag_key\n",
    "                    if tag_type:\n",
    "                        tag[\"type\"] = tag_type\n",
    "                    else:\n",
    "                        tag[\"type\"] = 'regular'\n",
    "                    tag[\"id\"] = element.attrib[\"id\"]\n",
    "                    tag[\"value\"] = child.attrib[\"v\"]\n",
    "    \n",
    "                else:\n",
    "                    tag[\"value\"] = child.attrib[\"v\"]\n",
    "                    tag[\"key\"] = child.attrib[\"k\"]\n",
    "                    tag[\"type\"] = \"regular\"\n",
    "                    tag[\"id\"] = element.attrib[\"id\"]\n",
    "                if tag:\n",
    "                    tags.append(tag)\n",
    "                    \n",
    "            elif child.tag == 'nd':\n",
    "                nd['id'] = element.attrib[\"id\"]\n",
    "                nd['node_id'] = child.attrib[\"ref\"]\n",
    "                nd['position'] = len(way_nodes)\n",
    "            \n",
    "                if nd:\n",
    "                    way_nodes.append(nd)\n",
    "            else:\n",
    "                continue\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "         codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_map('sample_sf.osm', validate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### SQL Queries and Data Exploration:\n",
    "\n",
    "** File Sizes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "san-francisco_california.osm ..........1,377.6 MB\n",
    "sample_sf.osm..........................57.7 MB\n",
    "osm.db.................................40.3 MB\n",
    "nodes.csv..............................22 MB\n",
    "nodes_tags.csv.........................385 KB\n",
    "ways.csv...............................2,008 KB\n",
    "ways_nodes.csv.........................7,628 KB\n",
    "ways_tags.csv..........................2,410 KB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of nodes**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT COUNT(*) FROM Nodes;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "265095"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of ways**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT COUNT(*) FROM Ways;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33062"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of Unique Users**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT COUNT(DISTINCT(e.uid))\n",
    "   ...> FROM (SELECT uid FROM Nodes UNION ALL SELECT uid FROM Ways) e;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1293"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 Contributing Users**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT e.user, COUNT(*) as num\n",
    "   ...> FROM (SELECT user FROM Nodes UNION ALL SELECT user FROM Ways) e\n",
    "   ...> GROUP BY e.user\n",
    "   ...> ORDER BY num DESC\n",
    "   ...> LIMIT(10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "andygol|59833\n",
    "ediyes|35482\n",
    "Luis36995|27207\n",
    "dannykath|21880\n",
    "RichRico|16534\n",
    "Rub21|15278\n",
    "calfarome|7625\n",
    "oldtopos|6728\n",
    "KindredCoda|6144\n",
    "karitotp|5560"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Number of Users Making Only 1 Contribution**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT COUNT(*) FROM\n",
    "   ...> (SELECT e.user, COUNT(*) as num\n",
    "   ...> FROM(SELECT user FROM Nodes UNION ALL SELECT user FROM Ways) e\n",
    "   ...> GROUP BY e.user\n",
    "   ...> HAVING num = 1) u;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 Most Visited Areas**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT value, COUNT(*) as num\n",
    "   ...> FROM Nodes_Tags\n",
    "   ...> WHERE key = 'amenity'\n",
    "   ...> GROUP BY value\n",
    "   ...> ORDER BY num DESC\n",
    "   ...> LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which apparently are restaurants.  Make sense...but toilets have only a count of 15?  Something seems off..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "restaurant|122\n",
    "cafe|48\n",
    "bench|47\n",
    "post_box|32\n",
    "place_of_worship|27\n",
    "fast_food|26\n",
    "school|25\n",
    "bicycle_parking|21\n",
    "toilets|15\n",
    "bank|14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which makes me wonder what are the top restaurants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT Nodes_Tags.value, COUNT(*) as num\n",
    "   ...> FROM Nodes_Tags\n",
    "   ...> JOIN (SELECT DISTINCT(id) FROM Nodes_Tags WHERE value = 'restaurant') i\n",
    "   ...> WHERE Nodes_Tags.id = i.id\n",
    "   ...> AND Nodes_Tags.key = 'name'\n",
    "   ...> GROUP BY Nodes_Tags.value\n",
    "   ...> ORDER BY num DESC\n",
    "   ...> LIMIT(15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31st Union|1\n",
    "ABC Cafe Restaurant|1\n",
    "Agave Uptown|1\n",
    "Albany Bowl Cafe|1\n",
    "American Kitchen|1\n",
    "Amici's East Coast Pizzeria|1\n",
    "Arinell Pizza|1\n",
    "AsiaSF|1\n",
    "BC Deli Sandwiches|1\n",
    "BEL|1\n",
    "Bar Tartine|1\n",
    "Bistro Burger|1\n",
    "Britt-Marie's|1\n",
    "Bubba Gump Shrimp Company|1\n",
    "Buffalo Wild Wings|1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly our dataset seems to containly only single values per restaurant, so we can't determine what is the most common eatery.  Which makes me curious because the last I checked San Francisco was a pretty dense city.  So, within our Nodes_Tags table how many actually 'keys' are there with the value 'name'?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT COUNT(*) FROM Nodes;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "265095"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlite> SELECT COUNT(*) FROM Nodes_Tags WHERE key = 'name';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "741"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27952243535336385"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(741/265095.0) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only **0.28%** of 'key' values are a name?  Okay, so it's clear that our dataset is indeed lacking on pieces of information that would help improve and provide a more representive map of the city.  To fill in these data gaps, the OpenStreetMap for the San Francisco area could actually pull information from the city's own datasets of the city and county, <https://datasf.org/opendata/>.  Viewing the site, the city has datasets that include geographic locations and boundaries, housings & buildings, and city infrastructure.  We would assume the state has detailed data of itself but there is one caveat to this idea in that because OSM and DATASF are two different databases maintained by different organizations, it should be expected that some tag conventions would be different and would take extra work trying to match the conventions and extract the information.  Fortunately, as we learned in this project, the task could be done programmatically.    \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "Overall the San Francisco area dataset seems pretty clean with a few inconsistencies in the zipcode format and street name abbrevations, which shouldn't be surprising for a community-based data project where people have their individual preferences.  With that said, the overall process for data wrangling and cleaning is NOT at all a simple task and takes a lot of time to properly complete.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "* OSM XML Wiki Documentation:  <https://wiki.openstreetmap.org/wiki/OSM_XML>\n",
    "* SQL Tutorial:  <https://www.techonthenet.com/sql/index.php>\n",
    "* Udacity OpenStreetMap Sample Project:   <https://gist.github.com/carlward/54ec1c91b62a5f911c42#file-sample_project-md>\n",
    "* DATASF: <https://datasf.org/opendata/>\n",
    "* All the Udacity mentors.  OMG thank you for getting me out of some of them dark black holes of figuring out code.  :D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
